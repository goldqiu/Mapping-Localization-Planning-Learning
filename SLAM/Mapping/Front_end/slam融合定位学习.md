# 前言

## 导读	

![1648437716(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648437716(1).jpg)

![1648437753(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648437753(1).jpg)

## 激光slam

**1. 地图**

对激光slam进行分类，最好的分类方式就是根据地图的种类来分，地图的表示形式直接关系了其他模块的可选项，甚至直接决定了整个系统的程序架构。

目前主流的地图表示形式有以下几种：

1）直接的点云地图。

2）占据栅格地图

3）概率地图

4）surfel地图

5）三维网格地图

6）Multi-level surface map

7）appearance-based map

**2. 前端**

前端是一个slam系统最基础的部分，它提供每一帧点云对应的位姿。在一个slam系统中，选择哪种前端，其实在选择了地图表示方法之后就基本确定了，可以大致梳理下可选的前端。

1）ICP、NDT以及层出不穷的各种改进版。

2）基于线和面特征。最典型的莫过于鼎鼎大名的LOAM了。

3）基于栅格。

4）基于range image。把点云展开成图片，借鉴二维图片中的方式更新位姿。

5）基于特征点和描述子。除了提取特征点以外，还计算对应的描述子。

6）基于语义特征。就是完全具有语义信息的点云，这样好处是不容易陷入局部最优了，但提语义也不是一件容易事。

**3. 闭环**

对于三维激光slam来讲，闭环是一个太重要的话题了，即便到现在，也是一个没有被完全解决的问题。其中最关键的核心在于当前端误差累计达到一定程度以后，场景重识别和位姿匹配都不能再以开环位姿为基础做判断，需要全局搜索和匹配了，目前主流的方法有以下几种：

1）基于栅格和分支定界。典型的就是cartographer中的闭环检测了。

2）基于appearance-based map的重识别，不过目前的多数这种方法都结合了激光和图像，对于只依赖激光的则不多。

3）基于特征点和描述子。特征点有了描述子，就可以全局搜索和匹配了，可以同时解决重识别和匹配问题。

4）基于直方图。对点云提取直方图，作为特征。

5）结合NDT。NDT每个网格的方差阵是可以提取特征的，这种特征也可以用来识别。

6）语义特征。最高级的特征好像什么都可以解决，不过现实中能不能提取出足够数量的完整语义特征倒是另外一回事。

**4. 后端**

使用图优化做后端已经成为当之无愧的首选，滤波的方法已经越来越少了。

**5. 信息融合**

对于自动驾驶中的激光slam来讲，除了优化前端和闭环产生的信息之外，大多数时候还要加上RTK和IMU，这样才能得到可复用的地理系下的地图。

RTK就是直接在因子图中作为先验边加入，而IMU则有多种用途，最合理的融合方式是使用预积分，不过略显复杂，退而求其次，则可与RTK先做一个组合，再作为先验加入因子图也可。

IMU的另外一个作用就是去畸变，对于机械雷达，这是基本要求。

**6. 定位**

自动驾驶中很少边建图边定位，一般都是建好点云地图，再在该地图上做定位，把当前帧和地图做匹配可以实现基本的定位，如果要更稳定，则需要加上融合。

**7. 初始定位**

把初始定位从定位中单独提出来，是因为它重要，我们建的图都是大场景地理系下的地图，最可靠的方式是RTK提供准确的位置，以此位置为基础可实现初始化。有些时候卫星信号不理想，则只能由RTK提供粗略位置，然后使用点云匹配的方式在一定附近区域内搜索得到最优的初始结果。此处搜索方式和闭环相似，基本可以互相借鉴。对于建图时没有使用卫星信号的情况，也可以使用这种方式进行全局搜索初始化。

## 视觉语义slam

语义信息对视觉slam的贡献，或者说视觉slam与语义结合的方式，主要可以分为以下几种

1. **用于改善特征选择**

传统方法提取描述子来进行位姿匹配，其中包含了动态物体或可能运动的物体的描述子，语义方法可以帮助识别出这些物体，并剔除相应的特征，提高鲁棒性。另外，有些方法中，神经网络输出的特征可以代替sift或者orb等描述子，用于位姿匹配。

**2. 用于位姿优化**

主要是利用语义识别出的物体或特征，进行数据关联，从而改善位姿的估计

**3. 用于重定位**

视觉slam的一个主要问题是所建立的地图会随着时间的推移或者环境的变化而逐渐不试用，这种变化多是光照等环境变化引起的。如果能够建立给予语义特征或语义识别的物体的地图，则会在此方面有很大的改善，尤其是面对life-long的需求的时候。

**4. 用于恢复尺度**

由于单目slam中缺少尺度信息，因此有很多语义slam方法致力于用深度学习去恢复单目相机的深度信息。

**5. 用于端到端位姿输出**

这种是直接基于图像信息对6自由度位姿进行端到端的输出。

# 总述

## 包含内容

1）3D激光SLAM

包括激光点云地图的建立与基于点云地图的定位。

建图又包括室外大场景建图与室内场景建图，区别是室外大场景必须借助RTK作为约束才能建立精确地图，室内场景由于没有RTK信号，则需要寻找其他约束才能建立闭合地图，比如闭环检测等。

2）点云地图定位基础上添加 IMU 、GNSS、轮速计等信息

仅依赖激光雷达定位，容易受外界环境影响，比如雷达被遮挡，或者进入了空旷地带，所以需要寻找在特性上和它互补的传感器做融合。

3）组合导航

此处组合导航是指“IMU+GNSS”的组合（轮速计一般作为附加项供用户选择，此处我们也会介绍相应的融合方法）。理论上，有了“点云地图定位+IMU+GNSS”的融合，就不再需要“IMU+GNSS”的组合导航系统了，此处仍然把它作为一个模块来做，是由自动驾驶的现状决定的，具体来讲就包括这样几个方面：

- 早期自动驾驶很多借助于组合导航系统来做定位，导致后来转成多传感器融合以后，很多都是购买一套“IMU+GNSS”的导航系统成品，然后再在此基础上加上雷达定位做融合。所以对组合导航的原理和具体实现做深入了解是很有必要的。
- 有些场景下的自动驾驶是使用雷达做定位的，所以这时候即使自己开发，能使用的传感器也只是IMU、GNSS、轮速计这些，所以这仍然是组合导航的范畴。

4）后处理

诺瓦泰的PP7在自动驾驶里的出镜率很高，如果使用过，那么对它的后处理软件“IE”应该也不陌生。后处理这个东西简单来讲就是给融合系统增加了将来视角，它会根据采集的数据对历史时刻的导航结果做修正，这样做的主要目的是在建立地图的时候使用，以对点云的位姿进一步优化，获得更高的地图精度。

## 系列风格

**1）会横向对比多种方法**

比如激光SLAM的前端会使用ndt、icp、特征匹配等方式各实现一遍，滤波会对比卡尔曼滤波、粒子滤波等方法，后端优化也会使用g2o、gtsam、ceres各实现一遍去对比，通过对比才能知道他们之间的区别，进而在以后的使用中知道在什么情况下选用什么方法。

**2）会展示完整过程**

关于这一点，我要重点解释一下，不然以后可能会被喷。

我们在软件开发过程中，会逐渐地发现问题、解决问题，直到开发结束，得到一个成熟的系统，这个系统的框架和开发之前的设计会有很大差别。这时候如果再让你开发一套类似的系统，你可能会直接按照以前的经验，就直接设计一套成熟的框架了。

这里的展示完整过程是指，我之前设计过这类系统，但是在本系列里，我不想直接给出最终的成熟框架，而是更倾向于再展示一遍第一次开发时遇到的那些问题。这是因为对于没有开发过这类系统的读者来讲，直接给他一个成熟框架，它可能用起来顺手，但是它不清楚为什么这么写，而我们从最简单的想法开始写起，随着进度的推进，系统的逐渐扩大，遇到问题时我们再针对问题去改进之前的简单框架，这样所有的读者就会知其然而且知其所以然，明白框架中每一个设计细节背后的原因。

这会导致一个问题，就是各个时期的版本在架构上可能会差距比较大，代码读起来要多花些精力，可能会默默在心里飙脏话，这也是要重点解释这一点的原因。

**3）使用ROS作为调试环境**

对于ROS的各种诟病想必各位也都了解，很多公司如果人手足够，都自己开发框架了。这里仍然使用它来作为调试环境，是因为如果抛弃ROS就要自己写环境，包括显示系统等，也就是要自己造轮子，而这个轮子其实和我们重点介绍的融合算法关系不大，我希望我们能把主要精力用在算法上。

# 数据集-Tag2.0

## 一、数据集介绍

### 1. 整体介绍

做算法，首先要先搞数据，开源代码自然用公开的数据集最好，本系列文章的代码就选KITTI数据集为测试数据。

KITTI数据集由德国卡尔斯鲁厄理工学院和丰田美国技术研究院联合创办，是目前国际上最大的专门面向自动驾驶的数据集。该数据集用于评测激光里程计(lidar odometry)，视觉里程计(visual odometry)，立体图像(stereo)，光流(optical flow)，3D物体检测(object detection)和3D跟踪(tracking)等算法在车载环境下的性能。KITTI包含市区、乡村和高速公路等场景采集的真实图像数据。

### **2. 采集平台**

一张图可以解释。

![slam-4](L:\寒假备份\算法学习\slam融合定位学习\图片\slam-4.jpg)

这张图可以看出它的传感器组成，包括一个64线激光雷达，在车顶的正中心，激光雷达两侧各放一个彩色摄像头和一个黑白摄像头，也就是一共四个摄像头。在雷达左后方，有一个组合导航系统（OXTS RT 3003），它可以输出RTK/IMU组合导航结果，包括经纬度和姿态，同时也输出IMU原始数据。

各个传感器之间的安装关系如下图所示

![slam-5](L:\寒假备份\算法学习\slam融合定位学习\图片\slam-5.jpg)

根据这张图可以很容易计算各个传感器之间，以及他们和车之间的相对位移。不过不需要自己计算，KITTI为每组数据都提供了对应的标定文件。

### **3. 选择与使用**

我们的主要目的是利用激光雷达点云数据和RTK数据做融合，所以只使用数据集里的RawData数据，它包含RTK、IMU、激光雷达、摄像头等传感器的数据和他们之间的标定关系，并且时间戳已经对应好。

由于我们是使用ROS作为调试环境，所以我把一部分数据转成了ROS的bag文件，放在百度网盘里，如果不想自己转换数据，就可以直接下载这里面的bag文件使用，文件在“转换后的bag文件/2011_10_03”文件夹里。由于百度网盘单个文件大小有限制，所以我做了分卷压缩，下载完成之后需要在当前目录下输入如下指令，把他们再合成一个文件才能解压

```text
cat bag_file*>bag.tar.gz
```

本文章后半部分会介绍数据转换的方法，如果想用数据集里其他数据做测试，可以根据介绍的方法自行生成bag文件。

## 二、KITTI数据转成ROS的bag文件

转换是通过一个开源工具kitti2bag来完成，它可以通过pip安装。

### 1. 升级numpy

这一步很重要，不然后面运行会报错。kitti2bag要求numpy版本>=1.12，ubuntu 16.04默认的是1.11，升级可以通过一条指令来完成

```text
sudo pip install -U numpy
```

升级之后，我的版本是1.16

### 2. 安装kitti2bag

也是一条指令

```text
sudo pip install kitti2bag
```

如果遇到问题，可以上网查一查，一般都是常见问题，都很好解决。

### 3. 下载文件并设置目录

从我给的百度网盘链接里找到“RawData原始数据”文件夹，找到文件“2011_10_03_drive_0027_sync.zip”和文件“2011_10_03_calib.zip”，下载下来，并解压缩。

最终文件应该按照这样的目录存放

![slam-6](L:\寒假备份\算法学习\slam融合定位学习\图片\slam-6.jpg)

### 4. 转换文件

在系统中打开终端，cd进入上一步目录对应的"2011_10_03"文件夹的上一级目录，输入下面的指令，就会自动开始转换

```text
kitti2bag -t 2011_10_03 -r 0027 raw_synced
```

如果指令正常执行，会出现下面的画面，耐心等他执行完就好了

执行结束之后，会生成一个文件“kitti_2011_10_03_drive_0027_synced.bag”，这个就是使用KITTI数据集生成的bag文件了。

## 三、bag文件测试

制作完bag文件之后，要播放文件测试以下，看显示出来的数据对不对。

我们就直接用rviz了。制作好的rviz文件放在了github上的代码里，在kitti_test文件夹下。

执行步骤如下：

1) 启动ros

```text
roscore
```

2) 启动rviz

打开终端，cd到“kitti_test”文件夹下，输入指令

```text
rviz -d display_bag.rviz
```

3) 播放bag

再次强调一下

由于百度网盘单个文件大小有限制，所以我做了分卷压缩，需要下载文件夹中bag_file所有的分卷，即bag_file00,bag_file01…,下载完成之后需要在当前目录下输入如下指令，把他们再合成一个文件才能解压

cat bag_file*>bag.tar.gz

打开终端，cd到bag所在目录，输入指令

```text
rosbag play kitti_2011_10_03_drive_0027_synced.bag
```





# 软件框架-Tag3.0

## **一、概述**

为了写好我们这套定位系统，框架自然是首要考虑的事情，设计框架要结合需求和配置环境针对性地设计。

此处我们的环境就是ROS系统，而需求就是从我们上一篇文章制作的bag文件中接收各传感器信息和标定信息，以供算法使用，在算法运算完成以后把结果发送出去。

所以我们的基础框架就包括接收模块、发送模块，以及与将来编写的算法对接的接口。

接收模块，具体包括接收bag文件中GNSS信息、IMU信息、雷达点云信息和各传感器之间的标定信息。

发送模块，具体包括发送当前点云、全局地图、局部地图、里程计信息、载体运动轨迹等。

接口，具体就是要设计合理的数据结构，能够把算法需要的输入信息和算法的输出信息条例合理地规划好，以使输入输出更清晰方便。

除了以上功能以外，我们还需要一些小的技巧，以使工程结构更清晰，文件中代码也更清晰。当然其中一些可能只是我个人的一些使用习惯，各位如果有更好的习惯也欢迎交流。

本篇文章对应的工程包是工程文件中的lidar_localization

## **二、ROS工程设计**

这一部分是本篇文章的重点，但是关于怎样使用ROS建立一个简单的工程包，这里不做详细介绍，网上的资料随处可见，了解不太多的读者麻烦先看一些资料，抱歉。

为了使工程架构更符合任务需求，我们对工程做了一些改动，此处重点要介绍的就是这些改动部分，这可能和大家经常见到的工程包的结构不一样。

### 1. 消息的订阅和发布

消息的订阅和发布大家应该不会陌生，这是每个ROS工程都必备的东西，我们常见的使用方式是在main函数中定义subscriber和publisher，每个subscriber会有一个callback函数与之对应。

这种使用方式会带来一些问题，那就是如果订阅的topic比较多，那这个node文件就会充斥大量的callback函数，而且如果有些信息需要在callback内部做比较多的解析和处理，那这个node文件的代码长度会很长，这会影响程序的清晰度。

针对这个问题，我们把每一类信息的订阅和发布封装成一个类，它的callback做为类内函数存在，这样我们在node文件中想要订阅这个消息的时候只需要在初始化的时候定义一个类的对象，就可以在正常使用过程中从类内部直接取它的数据了。

这样用文字说可能比较抽象，我们找其中一个订阅类来举例子。

就用订阅GNSS信息的例子好了，代码中，它的头文件是gnss_subscriber.hpp，源文件是gnss_subscriber.cpp。在头文件中，类的声明如下

```text
class GNSSSubscriber {
  public:
    GNSSSubscriber(ros::NodeHandle& nh, std::string topic_name, size_t buff_size);
    GNSSSubscriber() = default;
    void ParseData(std::deque<GNSSData>& deque_gnss_data);

  private:
    void msg_callback(const sensor_msgs::NavSatFixConstPtr& nav_sat_fix_ptr);

  private:
    ros::NodeHandle nh_;
    ros::Subscriber subscriber_;

    std::deque<GNSSData> new_gnss_data_;
};
```

其中msg_callback就是它的callback函数，也就是接收和处理信息的地方，它在源文件中的实现如下

```text
void GNSSSubscriber::msg_callback(const sensor_msgs::NavSatFixConstPtr& nav_sat_fix_ptr) {
    GNSSData gnss_data;
    gnss_data.time = nav_sat_fix_ptr->header.stamp.toSec();
    gnss_data.latitude = nav_sat_fix_ptr->latitude;
    gnss_data.longitude = nav_sat_fix_ptr->longitude;
    gnss_data.altitude = nav_sat_fix_ptr->altitude;
    gnss_data.status = nav_sat_fix_ptr->status.status;
    gnss_data.service = nav_sat_fix_ptr->status.service;

    new_gnss_data_.push_back(gnss_data);
}
```

类中的函数ParseData就是实现从类里取数据的功能，在源文件中的实现如下

```text
void GNSSSubscriber::ParseData(std::deque<GNSSData>& gnss_data_buff) {
    if (new_gnss_data_.size() > 0) {
        gnss_data_buff.insert(gnss_data_buff.end(), new_gnss_data_.begin(), new_gnss_data_.end());
        new_gnss_data_.clear();
    }
}
```

经过这样的改造，我们在node文件中使用它时，只需要完成类对象定义和取数据两步

```text
// 定义类对象指针
std::shared_ptr<GNSSSubscriber> gnss_sub_ptr = std::make_shared<GNSSSubscriber>(nh, "/kitti/oxts/gps/fix", 1000000);
ros::Rate rate(100);
while (ros::ok()) {
    ros::spinOnce();
    //取数据
    gnss_sub_ptr->ParseData(gnss_data_buff);
    rate.sleep();
}
```

这样node文件中代码量就会大大减少，使程序更清晰。

### 2. 传感器数据结构

每种传感器专门封装了对应的数据结构，在sensor_data文件夹下，目前有imu_data.hpp、gnss_data.hpp、cloud_data.hpp分别对应IMU数据、GNSS数据、点云数据。

这种封装就是为了适应一开始提到的接口功能，同时也可以配合第一步封装的订阅类和发布类使用，把订阅的数据直接封装好再供主程序取，这样封闭性更强。

### 3. 缓冲区机制

这种机制完全是由于ROS自身的缺陷导致的，而且我在以前的试验中也多次遇到过这个问题。

这个问题和ROS订阅信息时缓冲区读取有关，ROS在每次循环时，会逐个遍历各个subscriber的缓冲区，并且把缓冲区中的数据读完，不管有多少。我们在subscriber的callback中解析数据的时候，一般都是把数据赋给一个变量，然后在融合的时候使用最后更新的值作为输入。

如果觉得不好理解，我们使用伪代码举一个小例子，假如目前有雷达和GNSS信息，我们要融合它。

```text
gnss_callback {
  gnss 数据解析，赋给变量 gnss_data
}
lidar_callback {
  雷达数据解析，得到lidar_data
  融合(lidar_data, gnss_data)
}
```

这样看好像没什么问题，问题在于当融合算法处理时间比较长，超出了传感器信息的发送周期的时候，未被接收的数据会被放在每个subscriber对应的缓冲区中，等当前融合步骤处理完之后，下次ros从缓冲区中读取数据的时候，会先把gnss的数据读完，然后再读lidar的数据，这就导致，我们再一次进入lidar_callback函数时，使用的gnss_data已经不是和这个lidar_data同一时刻的数据了，而是它后面时刻的数据。

（不知道我有没有讲清楚，如果还没讲清楚就评论区留言吧）

为了解决这一问题，办法也很简单，就是我们不用单个变量来存储数据，而是用容器。各位这时候可以去第一步看我们举的那个GNSS信息订阅类的例子，在它的msg_callback函数里，信息解析完之后是放在一个deque容器里的。

这样算法再使用数据的时候，应该从容器中去找。只不过找的时候要注意，多个传感器产生了多个容器，往算法模块里输入的时候，应该按照各容器第一个数据的时间戳，把最早的那个输入送进去，循环这个过程，直到所有容器数据送完为止。

### 4. CMakeLists文件规划

这一部分介绍的都是一些小的使用习惯，我认为这些习惯可以使CMakeLists更清晰。

**1）把各个包放在单独的cmake文件中**

调用一个包，就是常规三步：find_package，include_directions，target_link_libraries

有时候还需要一些判断，就加一些if else。

这样也是同样的问题，包多的时候代码太杂。所以我们把每个包对应的这些操作放在cmake文件夹下对应的XX.cmake文件中，然后在CMakeLists中 include(cmake/XX.cmake)一行代码就可以搞定。

**2）合并变量**

为了避免target_link_libraries后面跟很长一串库的名字，而且库增减的时候它也得跟着增减，我们在CMakeLists文件一开始定义一个变量

```text
set(ALL_TARGET_LIBRARIES "")
```

然后在每个库对应的XX.cmake文件中，把库的名字合并到这个变量中去

```text
list(APPEND ALL_TARGET_LIBRARIES ${XX_LIBRARIES})
```

这样在target_link_libraries使就只使用ALL_TARGET_LIBRARIES这一个变量就好了。



除了库对应的变量，还有文件名字对应的变量，我们在add_executable的时候要把所需要的cpp文件路径都要写进去，文件多的时候也是太麻烦，所以可以使用下面的指令把所有cpp文件合并到一个变量中

```text
file(GLOB_RECURSE ALL_SRCS "*.cpp")
```

但是，当工程中有多个node文件的时候，就要把他们从这个变量中踢出去，因为多个node文件编到一个可执行文件中会出错。用下面的代码踢

```text
file(GLOB_RECURSE NODE_SRCS "src/*_node.cpp")
list(REMOVE_ITEM ALL_SRCS ${NODE_SRCS})
```

### 5. GLog使用

GLog是google开源的代码日志开源库，它把信息分为INFO、WARNING、ERROR几个等级，使用时如果想添加日志信息，只需要一行代码就可以了

```text
LOG(INFO) << "自定义日志信息";
```

日志信息会自动存储在你定义的目录中。

总之使用起来还是比直接使用std::cout要方便很多，我们就把它加入到这个工程里面来了，麻烦各位装一个吧。

## **三、一个小例程**

啰嗦这么多，我们要试一下刚才设计的这些东西好不好使，这个小功能就是在播放bag文件的同时，把采集数据时车的轨迹实时显示出来，并且把当前点云也显示在车当前的位置上。

基本思路就是订阅GNSS、IMU、lidar信息，然后把GNSS信息中的位置、IMU信息中的姿态信息解析出来，用odometry发布出去，再把订阅的点云信息按照解析的位姿数据转换到当前车的位置和方向上去，最后发布出去。

该功能对应的node文件是test_frame_node.cpp，文件内容很简单，而且上面的介绍也零散提到一些，此处就不放代码了。

编译完成之后运行程序

```text
roslaunch lidar_localization test_frame.launch
```





# 3D激光里程计

## 激光传感器原理

### Lidar的工作方式：

激光雷达传感器向周围环境发射脉冲光波；
这些脉冲碰撞到周围物体反弹并返回传感器；
传感器使用每个脉冲返回到传感器所花费的时间来计算其传播的距离；
每秒重复数百万次此过程，将创建精确的实时3D环境地图

![slam-1](L:\寒假备份\算法学习\slam融合定位学习\图片\slam-1.jpg)

![slam-2](L:\寒假备份\算法学习\slam融合定位学习\图片\slam-2.jpg)

### Lidar的分类：

机械旋转激光雷达(如vlp16)，固态激光雷达(如Livox Mid-40)

不同点：视角范围不相同

![slam-3](L:\寒假备份\算法学习\slam融合定位学习\图片\slam-3.jpg)

b. 扫描工作方式不同

机械旋转激光雷达: 一般是多个激光束同时发射，并绕固定轴旋转，来探测三维环境
缺点：远处的激光点之间的间隔较大

未来可能趋势：调频连续波（FMCW）激光雷达

## 整体流程介绍

![图片3](L:\寒假备份\算法学习\slam融合定位学习\图片\图片3.png)

提取关键性：稀疏

## 前端里程计方案

### ICP

![1648469859(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648469859(1).jpg)

![1648470080(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648470080(1).jpg)

怎么让两部分点云重合和如何评价重合。

3D-3D，需要找到两部分都有的点，找最近邻关联点。

最近邻关联点不断推进，去求解位姿，然后更新位姿再推进最近邻关联点，直到达到退出条件，输出最佳位姿。

问题：离得太远，很难迭代

需要初始位姿比较好。

![1648470382(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648470382(1).jpg)

ICP推导：根据代码中使用方法，去推导。



### NDT:

鲁棒性更强些，效率也更高

![1648471622(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648471622(1).jpg)

假设A点云静止不动，空间划分栅格。B点云通过R t旋转到A点云，正好落在栅格上，则说明匹配上了。

最终是两个点云的分布很接近。

ICP计算的是点对点，NDT是考虑高斯分布。

![1648472048(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648472048(1).jpg)

如果A B块点云之间旋转平移差太多，那么B块的均值必然和A块的均值差很多，那么联合概率就是最小的（负对数）



概率问题转化为非线性最小二乘问题，优化

关键是雅可比矩阵的推导！根据代码



方法的选用根据实际情况

### loam

![1648479697(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648479697(1).jpg)

![1648479971(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648479971(1).jpg)

曲率大：角点

曲率小：面点

曲率筛选是为了后面优化计算不同的残差方便

关键是残差的雅可比矩阵

即残差对位姿进行求导，残差的梯度下降的方向就是残差值减少的方向，也就是两帧点云越来越匹配上，其对应的位姿就是最终的位姿。

![1648480904(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648480904(1).jpg)

合并地图时，地图划分栅格，关键帧旋转到地图后，将对应栅格的关键帧的特征点进行合并。

![1648481111(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648481111(1).jpg)

F-LOAM:显示求导。使用李代数去求导。

A-LOAM：使用四元数去求导

原版LOAM是使用欧拉角。



### 基于数据集实现

![1648481683(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648481683(1).jpg)

![1648481766](L:\寒假备份\算法学习\slam融合定位学习\图片\1648481766.jpg)精度评估

![1648481993(1)](L:\寒假备份\算法学习\slam融合定位学习\图片\1648481993(1).jpg)



### LOAM代码

补畸变：匀速模型线性插值。

先分线（通过计算垂直角度）

计算曲率：前后五个点与中间做差。

与曲率公式有正相关关系

选四类曲率分特征。

 曲率特别大的作为线

曲率一般大的作为点线匹配的点

曲率特别小的作为面，

曲率一般小的作为点面匹配的点



曲率点六等分后再取特征点，特征点选取更稳定，更鲁棒。



然后进去里程计。

计算残差：点到面和点到线距离

计算雅可比：线和面对旋转平移求梯度（求导）

线（两个点）：找最近邻的一个点，往id增长和减少方向找。

找下一帧点云的最近邻点，计算残差

计算梯度，垂直于线（利用两个向量叉乘得到垂直于两个向量的向量）



面（三个点）：和线同样的步骤  



后端优化：地图分cube

根据cube找最近的5个点

找特征点利用特征值

两大一小为面特征（或者用共面约束），一大两小为线特征

找到计算梯度（垂直的法向量）,残差（点到线和面）

然后优化计算方式和前端一样

优化后得到新的位姿，根据落在cube中的关键帧特征点拼接到地图中去



### lego-loam

通过俯仰角来确定地面，俯仰角通过计算向量确认

广度优先遍历提取聚类

距离特别近的一堆点为一类，其他为离群点。

地图中有关键帧，方便后面回环检测。

跟全局定位一样的，需要有关键帧。

对于loam的地图是没有关键帧的概念的。



